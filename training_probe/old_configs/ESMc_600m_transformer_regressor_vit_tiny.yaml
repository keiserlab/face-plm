# Configuration for Hydra
hydra:
  run:
    dir: /tmp/hydra
  job:
    chdir: false

defaults:
  - data_config: ESMC_600m_full_embedding
  - wandb_config: base_wandb
  - _self_

train_config:
  _target_: face_plm.probes.utils.TrainConfig
  batch_size: 140
  learning_rate: 1e-3
  num_workers: 0
  profiler: simple
  gpu_num: 2
  max_epoch_number: 1000
  warmup_epochs: 50

lr_config:
  _target_: face_plm.probes.utils.HyperParameterScheduler
  train_config: ${train_config}
  final_value: 1e-8
  warmup_initial_value: 1e-7
  base_value: 1e-6
  warmup_epochs: ${train_config.warmup_epochs}

encoder_config:
  _target_: face_plm.probes.models.TransformerClsTokenEncoder
  num_layers: 12
  num_heads: 3
  input_dim: 1152 # ESMC embedding size
  dim_feedforward: 768

taskhead_config:
  _target_: face_plm.probes.models.RegressionHead
  input_dim: 1152

loss_config:
  _target_: torch.nn.MSELoss

wandb_config:
  run_name: ESMc_600m_full_embedding_vit_tiny

model_config:
  _target_: face_plm.probes.models.LightningModel
  optimizer: "Adam"
  wd: 0.00
  lr_scheduler: ${lr_config}
  encoder: ${encoder_config}
  task_head: ${taskhead_config}
  loss_func: ${loss_config}

  