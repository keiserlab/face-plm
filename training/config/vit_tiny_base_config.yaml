# Configuration for Hydra
hydra:
  run:
    dir: /tmp/hydra
  job:
    chdir: false

train_config:
  _target_: adk_dl.utils.TrainConfig
  batch_size: 64
  learning_rate: 1e-3
  num_workers: 0
  profiler: simple
  gpu_num: 0
  max_epoch_number: 1000
  warmup_epochs: 50

lr_config:
  _target_: adk_dl.utils.HyperParameterScheduler
  train_config: ${train_config}
  final_value: 1e-8
  warmup_initial_value: 1e-7
  base_value: 1e-6
  warmup_epochs: ${train_config.warmup_epochs}

encoder_config:
  _target_: adk_dl.models.TransformerClsTokenEncoder
  num_layers: 12
  num_heads: 3
  input_dim: 1280
  dim_feedforward: 768

taskhead_config:
  _target_: adk_dl.models.RegressionHead
  input_dim: ${encoder_config.input_dim}

loss_config:
  _target_: torch.nn.MSELoss

wandb_config:
  _target_: adk_dl.utils.WandbRunConfig
  project: adk_deep_learning
  run_name: ESM3_sm_open_v1-vit_tiny_64

model_config:
  _target_: adk_dl.models.LightningModel
  optimizer:  "Adam"
  wd: 0.00
  lr_scheduler: ${lr_config}
  encoder: ${encoder_config}
  task_head: ${taskhead_config}
  loss_func: ${loss_config}

  